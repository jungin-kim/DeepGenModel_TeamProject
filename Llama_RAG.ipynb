{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4f67b4-b83e-44f3-87cc-044e5f669c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 435 tensors from C:\\Users\\MCC\\.cache\\huggingface\\hub\\models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF\\snapshots\\9bf4892cf2017362dbadf99bd9a3523387135362\\ggml-model-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 48\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,40960]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,40960]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,40960]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   97 tensors\n",
      "llama_model_loader: - type q5_K:  289 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 261/40960 vs 260/40960 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 40960\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 48\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 34B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 10.80 B\n",
      "llm_load_print_meta: model size       = 7.13 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MCC\\.cache\\huggingface\\hub\\models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF\\snapshots\\9bf4892cf2017362dbadf99bd9a3523387135362\\ggml-model-Q5_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_tensors:        CPU buffer size =  7298.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.16 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1542\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '48', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\\n' + system_message + '<|im_end|>\\n'}}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\n",
      "' + system_message + '<|im_end|>\n",
      "'}}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <s>\n",
      "\n",
      "llama_print_timings:        load time =    7097.77 ms\n",
      "llama_print_timings:      sample time =      11.11 ms /    61 runs   (    0.18 ms per token,  5489.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7097.68 ms /    83 tokens (   85.51 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:        eval time =   23498.41 ms /    60 runs   (  391.64 ms per token,     2.55 tokens per second)\n",
      "llama_print_timings:       total time =   30653.37 ms /   143 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'text': 'A chat between a curious user and an artificial '\n",
      "                      'intelligence assistant. The assistant gives helpful, '\n",
      "                      \"detailed, and polite answers to the user's questions.\\n\"\n",
      "                      'Human: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”? ì•„ë˜ ì„ íƒì§€ ì¤‘ ê³¨ë¼ì£¼ì„¸ìš”.\\n'\n",
      "                      '\\n'\n",
      "                      '(A) ê²½ì„±\\n'\n",
      "                      '(B) ë¶€ì‚°\\n'\n",
      "                      '(C) í‰ì–‘\\n'\n",
      "                      '(D) ì„œìš¸\\n'\n",
      "                      '(E) ì „ì£¼\\n'\n",
      "                      'Assistant:\\n'\n",
      "                      'í•œêµ­ì€ ë™ì•„ì‹œì•„ì— ìœ„ì¹˜í•œ êµ­ê°€ì´ë©°, ê·¸ ìˆ˜ë„ëŠ” (D) ì„œìš¸ì…ë‹ˆë‹¤. ì„œìš¸ì€ ì„¸ê³„ì—ì„œ ê°€ì¥ í° ë„ì‹œ ì¤‘ '\n",
      "                      'í•˜ë‚˜ì´ì ê²½ì œì ìœ¼ë¡œ ì¤‘ìš”í•œ ì¤‘ì‹¬ì§€ë¡œì„œ, 2ì²œë§Œ ëª…ì´ ë„˜ëŠ” ì¸êµ¬ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ í•œêµ­ì˜ '\n",
      "                      'ì •ì¹˜, ë¬¸í™” ë° êµìœ¡ì˜ ì¤‘ì‹¬ì§€ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.'}],\n",
      " 'created': 1718237681,\n",
      " 'id': 'cmpl-5d41f5a0-ac43-45db-b924-b0c7a3a4654a',\n",
      " 'model': 'C:\\\\Users\\\\MCC\\\\.cache\\\\huggingface\\\\hub\\\\models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF\\\\snapshots\\\\9bf4892cf2017362dbadf99bd9a3523387135362\\\\ggml-model-Q5_K_M.gguf',\n",
      " 'object': 'text_completion',\n",
      " 'usage': {'completion_tokens': 60, 'prompt_tokens': 83, 'total_tokens': 143}}\n",
      "30.65631890296936\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# download model\n",
    "model_name_or_path = \"heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF\" # repo id\n",
    "# 4bit\n",
    "model_basename = \"ggml-model-Q5_K_M.gguf\" # file name\n",
    "\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "print(model_path)\n",
    "\n",
    "\n",
    "# CPU\n",
    "# lcpp_llm = Llama(\n",
    "#     model_path=model_path,\n",
    "#     n_threads=2,\n",
    "#     )\n",
    "\n",
    "# GPUì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ ì•„ë˜ ì½”ë“œë¡œ ì‹¤í–‰\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=4096, # Context window\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = 'í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”? ì•„ë˜ ì„ íƒì§€ ì¤‘ ê³¨ë¼ì£¼ì„¸ìš”.\\n\\n(A) ê²½ì„±\\n(B) ë¶€ì‚°\\n(C) í‰ì–‘\\n(D) ì„œìš¸\\n(E) ì „ì£¼'\n",
    "\n",
    "prompt = prompt_template.format(prompt=text)\n",
    "\n",
    "start = time.time()\n",
    "response = lcpp_llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    stop = ['</s>'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "pprint(response)\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545b14da-5f43-40e1-b2e7-25b4014e4bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 435 tensors from C:\\Users\\MCC\\.cache\\huggingface\\hub\\models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF\\snapshots\\9bf4892cf2017362dbadf99bd9a3523387135362\\ggml-model-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 48\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,40960]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,40960]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,40960]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   97 tensors\n",
      "llama_model_loader: - type q5_K:  289 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 261/40960 vs 260/40960 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 40960\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 48\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 34B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 10.80 B\n",
      "llm_load_print_meta: model size       = 7.13 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7298.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.16 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1542\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '48', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\\n' + system_message + '<|im_end|>\\n'}}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\n",
      "' + system_message + '<|im_end|>\n",
      "'}}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[í•œì‹] ê³ ê¹ƒì§‘ê¹ë‘ê¸°ëˆìœ¡ë³¶ìŒë°¥ ì‹œë˜ê¸°ëœì¥êµ­ ì‚¼ìƒ‰ê³„ë€ì°œ ì´ì•Œìƒˆì†¡ì´ë²„ì„¯ë³¶ìŒ ì¡°ë¯¸ê¹€ ë°°ì¶”ê¹€ì¹˜ *ì–‘ë°°ì¶”ìƒëŸ¬ë“œ+ë“œë ˆì‹±\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n6,000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   39045.77 ms\n",
      "llama_print_timings:      sample time =      49.90 ms /   256 runs   (    0.19 ms per token,  5130.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39045.60 ms /   166 tokens (  235.21 ms per token,     4.25 tokens per second)\n",
      "llama_print_timings:        eval time =   84102.91 ms /   255 runs   (  329.82 ms per token,     3.03 tokens per second)\n",
      "llama_print_timings:       total time =  123471.19 ms /   421 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'length',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'text': 'A chat between a curious user and an artificial '\n",
      "                      'intelligence assistant. The assistant gives helpful, '\n",
      "                      \"detailed, and polite answers to the user's questions \"\n",
      "                      'based on the provided context.\\n'\n",
      "                      '\\n'\n",
      "                      'Context: [í•œì‹] ê³ ê¹ƒì§‘ê¹ë‘ê¸°ëˆìœ¡ë³¶ìŒë°¥ ì‹œë˜ê¸°ëœì¥êµ­ ì‚¼ìƒ‰ê³„ë€ì°œ ì´ì•Œìƒˆì†¡ì´ë²„ì„¯ë³¶ìŒ ì¡°ë¯¸ê¹€ '\n",
      "                      'ë°°ì¶”ê¹€ì¹˜ *ì–‘ë°°ì¶”ìƒëŸ¬ë“œ+ë“œë ˆì‹±\\r\\n'\n",
      "                      '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n'\n",
      "                      '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n'\n",
      "                      '\\n'\n",
      "                      '6,000\\n'\n",
      "                      '\\n'\n",
      "                      'Human: ì˜¤ëŠ˜ì˜ ì ì‹¬ ë©”ë‰´ëŠ” ë¬´ì—‡ì´ê³  ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?\\n'\n",
      "                      'Assistant: ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ì ì‹¬ ë©”ë‰´ì™€ ê°€ê²©ì„ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ğŸ˜Š\\n'\n",
      "                      '\\n'\n",
      "                      'ì˜¤ëŠ˜ì˜ ì ì‹¬ ë©”ë‰´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n'\n",
      "                      '\\n'\n",
      "                      '* ê³ ê¹ƒì§‘ê¹ë‘ê¸°ëˆìœ¡ë³¶ìŒë°¥ (6,000ì›) - ë¶€ë“œëŸ¬ìš´ ë¼ì§€ê³ ê¸°ì™€ ë‹¬ì½¤í•œ ê¹ë‘ê¸°ê°€ ì–´ìš°ëŸ¬ì§„ ë§›ìˆëŠ” '\n",
      "                      'ìš”ë¦¬ì…ë‹ˆë‹¤.\\n'\n",
      "                      '* ì‹œë˜ê¸°ëœì¥êµ­ (5,000ì›) - êµ¬ìˆ˜í•œ ëœì¥ê³¼ ì˜ì–‘ê°€ ìˆëŠ” ì‹œë˜ê¸°ë¡œ ë§Œë“  ë“ ë“ í•œ êµ­ì´ì—ìš”.\\n'\n",
      "                      '* ì‚¼ìƒ‰ê³„ë€ì°œ (4,000ì›) - ë§›ê¹”ìŠ¤ëŸ¬ìš´ ìƒ‰ê°ê³¼ ë¶€ë“œëŸ¬ìš´ ì‹ê°ì„ ìë‘í•˜ëŠ” ê³„ë€ ìš”ë¦¬ì…ë‹ˆë‹¤.\\n'\n",
      "                      '* ì´ì•Œìƒˆì†¡ì´ë²„ì„¯ë³¶ìŒ (5,000ì›) - ìƒˆì½¤ë‹¬ì½¤í•˜ê²Œ ì¡°ë¦¬ëœ ë²„ì„¯ìœ¼ë¡œ ì…ë§›ì„ ë‹ìš°ëŠ” ìš”ë¦¬ì˜ˆìš”.\\n'\n",
      "                      '* ë°°ì¶”ê¹€ì¹˜ (ë¬´ë£Œ ì œê³µ) - ì‹ì‚¬ë¥¼ ë”ìš± ë§›ìˆê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë“ ë“ í•œ ë°˜ì°¬ì…ë‹ˆë‹¤.\\n'\n",
      "                      '* ì–‘ë°°ì¶”ìƒëŸ¬ë“œ+ë“œë ˆì‹± (2,000ì›) - ìƒí¼í•˜ê³  ê°€ë²¼ìš´ ìƒëŸ¬ë“œë¡œ ì‹ì „ì— ì…ë§›ì„ ë‹ìš°ì„¸ìš”.\\n'\n",
      "                      '\\n'\n",
      "                      'ì ì‹¬ ë©”ë‰´ ê°€ê²©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n'\n",
      "                      '\\n'\n",
      "                      '* ê³ ê¹ƒì§‘ê¹ë‘ê¸°ëˆìœ¡ë³¶ìŒë°¥'}],\n",
      " 'created': 1758529366,\n",
      " 'id': 'cmpl-740a9218-62c5-4710-8dab-cbf6a56e2f88',\n",
      " 'model': 'C:\\\\Users\\\\MCC\\\\.cache\\\\huggingface\\\\hub\\\\models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF\\\\snapshots\\\\9bf4892cf2017362dbadf99bd9a3523387135362\\\\ggml-model-Q5_K_M.gguf',\n",
      " 'object': 'text_completion',\n",
      " 'usage': {'completion_tokens': 256, 'prompt_tokens': 166, 'total_tokens': 422}}\n",
      "123.47815179824829\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import time\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
    "model_name_or_path = \"heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF\"\n",
    "model_basename = \"ggml-model-Q5_K_M.gguf\"\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "\n",
    "# GPU ì‚¬ìš©\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=8,  # CPU cores\n",
    "    n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=43,  # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=4096,  # Context window\n",
    ")\n",
    "\n",
    "# í™ˆí˜ì´ì§€ì—ì„œ ì ì‹¬ ë©”ë‰´ HTML ê°€ì ¸ì˜¤ê¸°\n",
    "url = \"https://www.hanyang.ac.kr/web/www/re15\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# ë©”ë‰´ ì •ë³´ ì¶”ì¶œ\n",
    "menu_elements = soup.find_all(\"h4\", class_=\"d-title2\")\n",
    "menus = []\n",
    "\n",
    "for menu_element in menu_elements:\n",
    "    if \"ì¤‘ì‹\" in menu_element.text:\n",
    "        menu_text = menu_element.find_next_sibling().text.strip()\n",
    "        menus.append(menu_text)\n",
    "\n",
    "print(menus)\n",
    "\n",
    "# ì ì‹¬ ë©”ë‰´ ë¬¸ì„œí™”\n",
    "menu_document = \"\\n\".join(menus)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì¤€ë¹„\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the provided context.\\n\\nContext: {context}\\n\\nHuman: {prompt}\\nAssistant:\"\n",
    "prompt = prompt_template.format(context=menu_document, prompt=\"ì˜¤ëŠ˜ì˜ ì ì‹¬ ë©”ë‰´ëŠ” ë¬´ì—‡ì´ê³  ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?\")\n",
    "\n",
    "start = time.time()\n",
    "response = lcpp_llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    stop=['</s>'],  # Dynamic stopping when such token is detected.\n",
    "    echo=True  # return the prompt\n",
    ")\n",
    "pprint(response)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04838ace-8da0-423f-9f58-f3a9f59ff33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 389 tensors from ./models/bge-m3-gguf/bge-m3-F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = bert\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                         general.size_label str              = 567M\n",
      "llama_model_loader: - kv   3:                            general.license str              = mit\n",
      "llama_model_loader: - kv   4:                               general.tags arr[str,4]       = [\"sentence-transformers\", \"feature-ex...\n",
      "llama_model_loader: - kv   5:                           bert.block_count u32              = 24\n",
      "llama_model_loader: - kv   6:                        bert.context_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                      bert.embedding_length u32              = 1024\n",
      "llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 4096\n",
      "llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                      bert.attention.causal bool             = false\n",
      "llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = t5\n",
      "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,250002]  = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \",\"...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,250002]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,250002]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = true\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.token_type_count u32              = 1\n",
      "llama_model_loader: - kv  21:    tokenizer.ggml.remove_extra_whitespaces bool             = true\n",
      "llama_model_loader: - kv  22:        tokenizer.ggml.precompiled_charsmap arr[u8,237539]   = [0, 180, 2, 0, 0, 132, 0, 0, 0, 0, 0,...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  26:          tokenizer.ggml.seperator_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.cls_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.mask_token_id u32              = 250001\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = true\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  244 tensors\n",
      "llama_model_loader: - type  f16:  145 tensors\n",
      "llm_load_vocab: unknown tokenizer: 't5'llm_load_vocab: using default tokenizer: 'llama'llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = bert\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 250002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 1024\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 4096\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 0\n",
      "llm_load_print_meta: pooling type     = 2\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 335M\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 566.70 M\n",
      "llm_load_print_meta: model size       = 1.07 GiB (16.25 BPW) \n",
      "llm_load_print_meta: general.name     = n/a\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "exception: access violation reading 0x0000000000000040",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 361\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 361\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/bge-m3-gguf/bge-m3-F16.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m                \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpooling_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLLAMA_POOLING_TYPE_MEAN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m                \u001b[49m\u001b[43muse_mmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(emb\u001b[38;5;241m.\u001b[39membed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m)))  \u001b[38;5;66;03m# ì°¨ì› ìˆ«ì ì°íˆë©´ ì„±ê³µ\u001b[39;00m\n\u001b[0;32m    367\u001b[0m     main()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ieie\\lib\\site-packages\\llama_cpp\\llama.py:338\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[1;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43m_LlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_ \u001b[38;5;241m=\u001b[39m tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ieie\\lib\\site-packages\\llama_cpp\\_internals.py:52\u001b[0m, in \u001b[0;36m_LlamaModel.__init__\u001b[1;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr(disable\u001b[38;5;241m=\u001b[39mverbose):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_load_model_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: exception: access violation reading 0x0000000000000040"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "RAG (llama.cpp ì„ë² ë”© ë²„ì „) - ë‹¨ì¼ íŒŒì¼\n",
    "\n",
    "í•„ìš” íŒ¨í‚¤ì§€:\n",
    "  pip install llama-cpp-python faiss-cpu beautifulsoup4 requests numpy huggingface_hub\n",
    "\n",
    "ê°œìš”:\n",
    "- (R) Retrieve: í•œì–‘ëŒ€ ì‹ë‹¨ í˜ì´ì§€ë¥¼ í¬ë¡¤/íŒŒì‹± â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ FAISS ì¸ë±ìŠ¤\n",
    "- (A) Augment: ì§ˆì˜ ì‹œ Top-K ê´€ë ¨ ì²­í¬ë§Œ ë½‘ì•„ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…\n",
    "- (G) Generate: llama.cpp ë¡œì»¬ LLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±\n",
    "\n",
    "í™˜ê²½ ì˜ì¡´ì„±:\n",
    "- TensorFlow/transformers/sentence-transformers ë¶ˆí•„ìš”\n",
    "- ì„ë² ë”©ì€ llama.cppì˜ embedding=True ëª¨ë¸ì„ ì‚¬ìš©(GGUF ì„ë² ë”© ì „ìš©)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime as dt\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from bs4 import BeautifulSoup\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# =========================\n",
    "# êµ¬ì„±(í•„ìš”ì‹œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
    "# =========================\n",
    "URLS = [\n",
    "    \"https://www.hanyang.ac.kr/web/www/re15\",  # í•œì–‘ëŒ€ ì‹ë‹¨(ì˜ˆì‹œ). í•„ìš”ì‹œ ì¶”ê°€/ë³€ê²½\n",
    "]\n",
    "\n",
    "INDEX_DIR = \"./rag_index_llamacpp\"\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "\n",
    "TTL_HOURS = 4            # ì¸ë±ìŠ¤ ì‹ ì„ ë„ TTL\n",
    "TOP_K = 8                # ê²€ìƒ‰ ìƒìœ„ ê°œìˆ˜\n",
    "CTX_TOKENS = 4096        # ì¶”ë¡  LLM ì»¨í…ìŠ¤íŠ¸\n",
    "\n",
    "# [1] ì¶”ë¡ ìš© LLM (í•œê¸€ ì§€ì‹œí˜•)\n",
    "GEN_REPO = \"heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF\"\n",
    "GEN_FILE = \"ggml-model-Q5_K_M.gguf\"\n",
    "\n",
    "# [2] ì„ë² ë”© ì „ìš© GGUF (ì˜ˆì‹œ: ì‚¬ìš©ìê°€ ë³´ìœ í•œ multilingual ì„ë² ë”© GGUFë¡œ êµì²´í•˜ì„¸ìš”)\n",
    "#     - bge/bge-m3 ê³„ì—´ GGUF ë³€í™˜ë³¸ ë“± (í•œêµ­ì–´ í¬í•¨ ë©€í‹°ë§êµ¬ì–¼ ê¶Œì¥)\n",
    "#     - ì•„ë˜ëŠ” \"ì§ì ‘ ë‹¤ìš´ë¡œë“œí•œ ë¡œì»¬ íŒŒì¼ ê²½ë¡œ\"ë¥¼ ì“°ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
    "EMBED_MODEL_PATH = r\"./models/bge-m3-gguf/bge-m3-F16.gguf\"   # <= ì—¬ê¸°ë¥¼ ì‹¤ì œ ì„ë² ë”© GGUF ê²½ë¡œë¡œ!\n",
    "\n",
    "# =========================\n",
    "# ìœ í‹¸\n",
    "# =========================\n",
    "def normalize_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "def save_jsonl(path: str, rows: List[Dict[str, Any]]):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "# =========================\n",
    "# llama.cpp ëª¨ë¸ ë¡œë“œ\n",
    "# =========================\n",
    "def load_generation_llm() -> Llama:\n",
    "    # ì¶”ë¡ ìš© LLMì€ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ë°›ì•„ì˜µë‹ˆë‹¤.\n",
    "    gen_path = hf_hub_download(repo_id=GEN_REPO, filename=GEN_FILE)\n",
    "    llm = Llama(\n",
    "        model_path=gen_path,\n",
    "        n_threads=8,\n",
    "        n_batch=512,\n",
    "        n_gpu_layers=43,   # GPU VRAMì— ë§ì¶° ì¡°ì •\n",
    "        n_ctx=CTX_TOKENS,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "from llama_cpp import Llama, LLAMA_POOLING_TYPE_MEAN\n",
    "\n",
    "def load_embedding_llm() -> Llama:\n",
    "    path = r\"./models/bge-m3-gguf/bge-m3-F16.gguf\"  # ì‹¤ì œ íŒŒì¼ëª…/ê²½ë¡œë¡œ ë°”ê¾¸ê¸°\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"ì„ë² ë”© GGUF ì—†ìŒ: {path}\")\n",
    "    emb_llm = Llama(\n",
    "        model_path=path,\n",
    "        embedding=True,\n",
    "        pooling_type=LLAMA_POOLING_TYPE_MEAN,  # â˜… ì¶”ê°€\n",
    "        n_threads=8,\n",
    "        n_ctx=0,\n",
    "        use_mmap=False,   # â˜… Windows íšŒí”¼ ì˜µì…˜(í•„ìš”ì‹œ Trueë¡œ ë°”ê¿”ë„ ë¨)\n",
    "        use_mlock=False,\n",
    "        verbose=True\n",
    "    )\n",
    "    return emb_llm\n",
    "\n",
    "\n",
    "# ì „ì—­(ì§€ì—° ë¡œë“œ)\n",
    "_GEN_LLM = None\n",
    "_EMB_LLM = None\n",
    "\n",
    "def get_gen_llm() -> Llama:\n",
    "    global _GEN_LLM\n",
    "    if _GEN_LLM is None:\n",
    "        _GEN_LLM = load_generation_llm()\n",
    "    return _GEN_LLM\n",
    "\n",
    "def get_emb_llm() -> Llama:\n",
    "    global _EMB_LLM\n",
    "    if _EMB_LLM is None:\n",
    "        _EMB_LLM = load_embedding_llm()\n",
    "    return _EMB_LLM\n",
    "\n",
    "# =========================\n",
    "# í¬ë¡¤/íŒŒì‹±\n",
    "# =========================\n",
    "def fetch_html(url: str) -> str:\n",
    "    r = requests.get(url, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding or \"utf-8\"\n",
    "    return r.text\n",
    "\n",
    "def parse_menu_page(html: str, url: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    - h4.d-title2 ì¤‘ 'ì¤‘ì‹' ì„¹ì…˜ì„ ì°¾ì•„ í•´ë‹¹ ë¸”ë¡ì—ì„œ í…ìŠ¤íŠ¸ ë¼ì¸ ìˆ˜ì§‘\n",
    "    - ê°€ê²©/ì‹ë‹¹ëª…/ë‚ ì§œ íŒíŠ¸ ì¶”ì¶œ\n",
    "    - í˜ì´ì§€ êµ¬ì¡° ë³€ê²½ì„ ê³ ë ¤í•´ ê´€ëŒ€í•˜ê²Œ íŒŒì‹±\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    items = []\n",
    "\n",
    "    for h4 in soup.find_all(\"h4\", class_=\"d-title2\"):\n",
    "        title = normalize_ws(h4.get_text(\" \"))\n",
    "        if \"ì¤‘ì‹\" not in title:\n",
    "            continue\n",
    "\n",
    "        container = h4.find_next_sibling()\n",
    "        if container is None:\n",
    "            container = h4.parent\n",
    "\n",
    "        texts = []\n",
    "        for tag in container.descendants:\n",
    "            if tag.name in [\"li\", \"p\", \"tr\", \"td\", \"div\", \"span\"]:\n",
    "                t = normalize_ws(tag.get_text(\" \"))\n",
    "                if len(t) >= 2:\n",
    "                    texts.append(t)\n",
    "\n",
    "        # ì¤‘ë³µ ì œê±°\n",
    "        seen, dedup = set(), []\n",
    "        for t in texts:\n",
    "            if t not in seen:\n",
    "                seen.add(t)\n",
    "                dedup.append(t)\n",
    "\n",
    "        # ë‚ ì§œ íŒíŠ¸(ì˜ˆ: 2025.11.04, 2025-11-04 ë“±)\n",
    "        date_hint = None\n",
    "        dm = re.search(r\"(20\\d{2}[./-]\\s?\\d{1,2}[./-]\\s?\\d{1,2})\", \" \".join(dedup))\n",
    "        if dm:\n",
    "            date_hint = dm.group(1).replace(\" \", \"\")\n",
    "\n",
    "        # ì‹ë‹¹ íŒíŠ¸\n",
    "        cafe_hint = None\n",
    "        for kw in [\"í•™ìƒì‹ë‹¹\", \"êµì§ì›ì‹ë‹¹\", \"ì°½ì˜ì¸ì¬ì›\", \"ìƒí™œê´€\", \"í•œì–‘í”Œë¼ì\", \"í‘¸ë“œì½”íŠ¸\", \"ì¹´í˜í…Œë¦¬ì•„\"]:\n",
    "            if kw in \" \".join(dedup):\n",
    "                cafe_hint = kw\n",
    "                break\n",
    "\n",
    "        # ê°€ê²© íŒ¨í„´\n",
    "        price_pat = re.compile(r\"(\\d{3,5})\\s*ì›\")\n",
    "        for line in dedup:\n",
    "            if len(line) < 4:\n",
    "                continue\n",
    "            pm = price_pat.search(line)\n",
    "            price = int(pm.group(1)) if pm else None\n",
    "            menu_name = normalize_ws(line.replace(pm.group(0), \"\")) if pm else line\n",
    "\n",
    "            items.append({\n",
    "                \"meal\": \"ì¤‘ì‹\",\n",
    "                \"menu\": menu_name,\n",
    "                \"price_krw\": price,\n",
    "                \"date_hint\": date_hint,\n",
    "                \"cafeteria\": cafe_hint,\n",
    "                \"source_url\": url,\n",
    "                \"raw\": line\n",
    "            })\n",
    "\n",
    "    return items\n",
    "\n",
    "# =========================\n",
    "# ì²­í‚¹/ì„ë² ë”©/ì¸ë±ìŠ¤\n",
    "# =========================\n",
    "def chunk_records(records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    for r in records:\n",
    "        text = f\"[{r.get('cafeteria') or 'ì‹ë‹¹?'} / {r.get('meal')}] {r['menu']}\"\n",
    "        if r.get(\"price_krw\"):\n",
    "            text += f\" - {r['price_krw']}ì›\"\n",
    "        chunks.append({\"text\": text, \"meta\": r})\n",
    "    return chunks\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    llama.cpp ì„ë² ë”©: ê° í…ìŠ¤íŠ¸ë¥¼ emb_llm.embed()ë¡œ ì¶”ì¶œ í›„ L2 ì •ê·œí™”\n",
    "    \"\"\"\n",
    "    emb_llm = get_emb_llm()\n",
    "    vecs = []\n",
    "    for t in texts:\n",
    "        e = emb_llm.embed(t)  # ë˜ëŠ” emb_llm.create_embedding(input=t)[\"data\"][0][\"embedding\"]\n",
    "        v = np.asarray(e, dtype=np.float32)\n",
    "        v /= (np.linalg.norm(v) + 1e-12)\n",
    "        vecs.append(v)\n",
    "    return np.vstack(vecs).astype(np.float32)\n",
    "\n",
    "def build_or_refresh_index(force: bool = False) -> Tuple[faiss.IndexFlatIP, List[Dict[str, Any]]]:\n",
    "    index_path = os.path.join(INDEX_DIR, \"faiss.index\")\n",
    "    meta_path  = os.path.join(INDEX_DIR, \"meta.jsonl\")\n",
    "\n",
    "    need_build = force\n",
    "    if os.path.exists(index_path) and os.path.exists(meta_path):\n",
    "        mtime = dt.datetime.fromtimestamp(os.path.getmtime(index_path))\n",
    "        if (dt.datetime.now() - mtime) > dt.timedelta(hours=TTL_HOURS):\n",
    "            need_build = True\n",
    "    else:\n",
    "        need_build = True\n",
    "\n",
    "    if not need_build:\n",
    "        index = faiss.read_index(index_path)\n",
    "        meta = load_jsonl(meta_path)\n",
    "        return index, meta\n",
    "\n",
    "    # ìƒˆë¡œ êµ¬ì¶•\n",
    "    all_records: List[Dict[str, Any]] = []\n",
    "    for url in URLS:\n",
    "        try:\n",
    "            html = fetch_html(url)\n",
    "            recs = parse_menu_page(html, url)\n",
    "            all_records.extend(recs)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Fetch/Parse failed for {url}: {e}\")\n",
    "\n",
    "    chunks = chunk_records(all_records)\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "\n",
    "    if not texts:\n",
    "        # ë¹„ìƒ: ë¹ˆ ì¸ë±ìŠ¤ ë°˜í™˜\n",
    "        # ì°¨ì›ì€ ì„ì‹œë¡œ 768ë¡œ ê°€ì •í•˜ì§€ ë§ê³ , ì„ë² ë”© í•œ ë²ˆ ì‹œë„í•´ì„œ ê²°ì •í•˜ëŠ” ê²Œ ê°€ì¥ ì•ˆì „.\n",
    "        # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ 'ë¹ˆ ì¸ë±ìŠ¤'ë¥¼ ë§Œë“¤ì–´ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        print(\"[WARN] ì¸ë±ì‹±í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        # embed í•œ ë²ˆë„ ëª» í–ˆìœ¼ë¯€ë¡œ ì°¨ì› ëª¨ë¦„ â†’ IP ì¸ë±ìŠ¤ ê¸°ë³¸ ìƒì„± ë¶ˆê°€\n",
    "        # ì°¨ì›ì„ ì•Œì•„ì•¼ í•˜ë¯€ë¡œ ì„ì‹œ ë”ë¯¸ ì„ë² ë”©:\n",
    "        dummy = embed_texts([\"dummy\"])\n",
    "        d = dummy.shape[1]\n",
    "        index = faiss.IndexFlatIP(d)\n",
    "        save_jsonl(meta_path, [])\n",
    "        faiss.write_index(index, index_path)\n",
    "        return index, []\n",
    "\n",
    "    X = embed_texts(texts)\n",
    "    d = X.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(X)\n",
    "\n",
    "    save_jsonl(meta_path, chunks)\n",
    "    faiss.write_index(index, index_path)\n",
    "    return index, chunks\n",
    "\n",
    "def retrieve(query: str, top_k: int = TOP_K) -> List[Dict[str, Any]]:\n",
    "    index, meta = build_or_refresh_index(force=False)\n",
    "    if index.ntotal == 0 or len(meta) == 0:\n",
    "        return []\n",
    "    qv = embed_texts([query])\n",
    "    D, I = index.search(qv, top_k)\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        item = dict(meta[idx])\n",
    "        item[\"score\"] = float(score)\n",
    "        results.append(item)\n",
    "    return results\n",
    "\n",
    "# =========================\n",
    "# í”„ë¡¬í”„íŠ¸/ìƒì„±\n",
    "# =========================\n",
    "SYSTEM_TMPL = (\n",
    "    \"ë‹¹ì‹ ì€ ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•˜ëŠ” í•œêµ­ì–´ ì¡°ìˆ˜ì…ë‹ˆë‹¤. \"\n",
    "    \"ì˜¤ì§ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ê³ , ëª¨í˜¸í•˜ë©´ ë¶ˆí™•ì‹¤í•¨ì„ ë¶„ëª…íˆ ë°íˆì„¸ìš”. \"\n",
    "    \"í•­ëª©ë³„ë¡œ ì •ë¦¬í•˜ê³  ê° í•­ëª© ëì— [ì¶œì²˜: ì‹ë‹¹/URL]ì„ í‘œê¸°í•˜ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "USER_TMPL = (\n",
    "    \"ì§ˆë¬¸: {q}\\n\\n\"\n",
    "    \"ì»¨í…ìŠ¤íŠ¸:\\n{ctx}\\n\\n\"\n",
    "    \"ìš”êµ¬ì‚¬í•­:\\n\"\n",
    "    \"- ì˜¤ëŠ˜ì˜ ì ì‹¬ ë©”ë‰´ì™€ ê°€ê²©ì„ í‘œë¡œ ê°„ë‹¨ ì •ë¦¬\\n\"\n",
    "    \"- ì—¬ëŸ¬ ì‹ë‹¹ì´ë©´ ì‹ë‹¹ë³„ë¡œ êµ¬ë¶„\\n\"\n",
    "    \"- ë‚ ì§œê°€ ë¶ˆëª…í™•í•˜ë©´ í˜„ì¬ ë‚ ì§œ({today}) ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ ì—…ë°ì´íŠ¸ë¡œ ê°„ì£¼í•˜ë˜, ë¶ˆí™•ì‹¤í•¨ì„ ëª…ì‹œ\\n\"\n",
    "    \"- ë§ˆì§€ë§‰ì— 1ì¤„ ìš”ì•½\"\n",
    ")\n",
    "\n",
    "def format_context(docs: List[Dict[str, Any]]) -> str:\n",
    "    lines = []\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        meta = d[\"meta\"]\n",
    "        lines.append(\n",
    "            f\"[{i}] {d['text']}  \"\n",
    "            f\"(ì‹ë‹¹={meta.get('cafeteria')}, ë‚ ì§œíŒíŠ¸={meta.get('date_hint')}, URL={meta.get('source_url')})\"\n",
    "        )\n",
    "    return \"\\n\".join(lines) if lines else \"ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "def generate_answer(query: str, retrieved: List[Dict[str, Any]]) -> str:\n",
    "    llm = get_gen_llm()\n",
    "    today = dt.datetime.now(dt.timezone(dt.timedelta(hours=9))).strftime(\"%Y-%m-%d\")  # Asia/Seoul\n",
    "    ctx = format_context(retrieved)\n",
    "    prompt = f\"{SYSTEM_TMPL}\\n\\nì‚¬ìš©ìì™€ì˜ ëŒ€í™”.\\n\\n\" + USER_TMPL.format(q=query, ctx=ctx, today=today)\n",
    "\n",
    "    out = llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=512,\n",
    "        temperature=0.3,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        stop=[\"</s>\"],\n",
    "        echo=False\n",
    "    )\n",
    "    return out[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# =========================\n",
    "# ë©”ì¸\n",
    "# =========================\n",
    "def main():\n",
    "    print(\">> ì¸ë±ìŠ¤ êµ¬ì¶•/ê°±ì‹  ì¤‘...\")\n",
    "    build_or_refresh_index(force=False)\n",
    "\n",
    "    user_q = \"ì˜¤ëŠ˜ì˜ ì ì‹¬ ë©”ë‰´ëŠ” ë¬´ì—‡ì´ê³  ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?\"\n",
    "    print(f\">> ê²€ìƒ‰ ì§ˆì˜: {user_q}\")\n",
    "    hits = retrieve(user_q, top_k=TOP_K)\n",
    "    if not hits:\n",
    "        print(\">> ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê°•ì œ ì¬êµ¬ì¶• í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.\")\n",
    "        build_or_refresh_index(force=True)\n",
    "        hits = retrieve(user_q, top_k=TOP_K)\n",
    "\n",
    "    print(\"\\n[Top-K ì»¨í…ìŠ¤íŠ¸]\")\n",
    "    for h in hits[:5]:\n",
    "        print(\"-\", h[\"text\"], \"| score=\", round(h[\"score\"], 3))\n",
    "\n",
    "    print(\"\\n>> ë‹µë³€ ìƒì„± ì¤‘...\")\n",
    "    answer = generate_answer(user_q, hits[:5])\n",
    "    print(\"\\n=== ìµœì¢… ë‹µë³€ ===\\n\")\n",
    "    print(answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    emb = Llama(model_path=r\"./models/bge-m3-gguf/bge-m3-F16.gguf\",\n",
    "                embedding=True,\n",
    "                pooling_type=LLAMA_POOLING_TYPE_MEAN,\n",
    "                use_mmap=False, use_mlock=False)\n",
    "    print(len(emb.embed(\"hello\")))  # ì°¨ì› ìˆ«ì ì°íˆë©´ ì„±ê³µ\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef5eb1f9-243f-4d56-8993-dda93952da93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 389 tensors from ./models/bge-m3-gguf/bge-m3-F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = bert\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                         general.size_label str              = 567M\n",
      "llama_model_loader: - kv   3:                            general.license str              = mit\n",
      "llama_model_loader: - kv   4:                               general.tags arr[str,4]       = [\"sentence-transformers\", \"feature-ex...\n",
      "llama_model_loader: - kv   5:                           bert.block_count u32              = 24\n",
      "llama_model_loader: - kv   6:                        bert.context_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                      bert.embedding_length u32              = 1024\n",
      "llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 4096\n",
      "llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                      bert.attention.causal bool             = false\n",
      "llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = t5\n",
      "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,250002]  = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \",\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying pooling = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,250002]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,250002]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = true\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.token_type_count u32              = 1\n",
      "llama_model_loader: - kv  21:    tokenizer.ggml.remove_extra_whitespaces bool             = true\n",
      "llama_model_loader: - kv  22:        tokenizer.ggml.precompiled_charsmap arr[u8,237539]   = [0, 180, 2, 0, 0, 132, 0, 0, 0, 0, 0,...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  26:          tokenizer.ggml.seperator_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.cls_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.mask_token_id u32              = 250001\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = true\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  244 tensors\n",
      "llama_model_loader: - type  f16:  145 tensors\n",
      "llm_load_vocab: unknown tokenizer: 't5'llm_load_vocab: using default tokenizer: 'llama'llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = bert\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 250002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 1024\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 4096\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 0\n",
      "llm_load_print_meta: pooling type     = 2\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 335M\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 566.70 M\n",
      "llm_load_print_meta: model size       = 1.07 GiB (16.25 BPW) \n",
      "llm_load_print_meta: general.name     = n/a\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "exception: access violation reading 0x0000000000000040",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pooling \u001b[38;5;129;01min\u001b[39;00m (LLAMA_POOLING_TYPE_MEAN, LLAMA_POOLING_TYPE_CLS):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying pooling =\u001b[39m\u001b[38;5;124m\"\u001b[39m, pooling)\n\u001b[1;32m----> 7\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpooling_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_mmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(emb\u001b[38;5;241m.\u001b[39membed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mping\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ieie\\lib\\site-packages\\llama_cpp\\llama.py:338\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[1;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43m_LlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_ \u001b[38;5;241m=\u001b[39m tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ieie\\lib\\site-packages\\llama_cpp\\_internals.py:52\u001b[0m, in \u001b[0;36m_LlamaModel.__init__\u001b[1;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr(disable\u001b[38;5;241m=\u001b[39mverbose):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_load_model_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: exception: access violation reading 0x0000000000000040"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama, LLAMA_POOLING_TYPE_MEAN, LLAMA_POOLING_TYPE_CLS\n",
    "\n",
    "PATH = r\"./models/bge-m3-gguf/bge-m3-F16.gguf\"  # ì‹¤ì œ íŒŒì¼ëª…\n",
    "\n",
    "for pooling in (LLAMA_POOLING_TYPE_MEAN, LLAMA_POOLING_TYPE_CLS):\n",
    "    print(\"Trying pooling =\", pooling)\n",
    "    emb = Llama(model_path=PATH, embedding=True,\n",
    "                pooling_type=pooling, use_mmap=False, use_mlock=False, verbose=True)\n",
    "    print(\"dim:\", len(emb.embed(\"ping\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd615b-a894-46df-97c8-6f563d0fd168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
